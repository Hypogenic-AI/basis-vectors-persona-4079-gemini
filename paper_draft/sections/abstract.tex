As Large Language Models (LLMs) are increasingly deployed in personalized and safety-critical roles, understanding the internal structure of their persona representations becomes essential. 
We investigate the global geometry of persona space in \\\\qwen by performing Principal Component Analysis (\\\\textsc{pca}) on activation-level persona vectors extracted from seven diverse behavioral domains, including \\\\sycophancy, \\\\survival, and \\\\coordination. 
Our analysis reveals a remarkably low-rank structure in the early-middle layers of the residual stream, where a single Principal Component (\\\\pcone) explains approximately 60\\% of the total variance across all behavioral datasets in Layer 14. 
This primary basis vector aligns most strongly with a cluster of social compliance and self-preservation traits, suggesting a shared ``Social-Self'' axis that serves as a fundamental building block for complex personas. 
We demonstrate that this basis vector is causally meaningful by using it to steer the model's behavior on an unseen behavioral task (\\\\refusal), showing shifts in response probabilities. 
Finally, we observe a layer-wise dissipation of this low-rank structure, with the variance concentration decreasing from 60\\% in middle layers to 20\\% in the final layers, indicating that persona-related information becomes increasingly specialized and distributed as it approaches the output head.